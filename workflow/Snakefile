configfile: "config/config.yaml"

workdir: config['workdir']

max_threads = config["max_threads_per_rule"]
samples = config["samples"]

def get_input_fastq(wildcards):
    return samples[wildcards.sample]["fastq"][wildcards.read]

def script_path(script_name):
    return str(workflow.source_path(script_name))

# onsuccess:
#     shell("rm -r temp/")

rule all:
    input: 
        expand("results/{sample}/fastqs/R1_trim.fastq.gz", sample=samples.keys()),
        expand("results/{sample}/fastqs/R2_trim.fastq.gz", sample=samples.keys()),
        expand("results/{sample}/mapping/raw.bam", sample=samples.keys()),
        expand("results/{sample}/mapping/raw.bam.bai", sample=samples.keys()),
        expand("results/{sample}/filter/filtered.bam", sample=samples.keys()),
        expand("results/{sample}/filter/filtered.bam.bai", sample=samples.keys()),
        expand("results/{sample}/fragments/fragments.tsv.gz", sample=samples.keys()),
        expand("results/{sample}/fragments/fragments.tsv.gz.tbi", sample=samples.keys()),
        expand("results/{sample}/fragments/fragments.tar", sample=samples.keys()),
        expand("results/{sample}/archr/archr_project.tar.gz", sample=samples.keys()),
        expand("results/{sample}/qc/barcode_matching.tsv", sample=samples.keys()),
        expand("results/{sample}/qc/trim_adapters.txt", sample=samples.keys()),
        expand("results/{sample}/qc/samstats_raw.txt", sample=samples.keys()),
        expand("results/{sample}/qc/frac_mito.txt", sample=samples.keys()),
        expand("results/{sample}/qc/markdup.txt", sample=samples.keys()),
        expand("results/{sample}/qc/pbc_stats.tsv", sample=samples.keys()),
        expand("results/{sample}/qc/samstats_filtered.txt", sample=samples.keys()),
        expand("results/{sample}/qc/archr_doublet_summary.pdf", sample=samples.keys()),
        expand("results/{sample}/qc/archr_doublet_summary.tsv", sample=samples.keys()),
        expand("results/{sample}/qc/archr_fragment_size_distribution.pdf", sample=samples.keys()),
        expand("results/{sample}/qc/archr_pre_filter_metadata.tsv", sample=samples.keys()),
        expand("results/{sample}/qc/archr_tss_by_unique_frags.pdf", sample=samples.keys())
        
"""
######################
FASTQ processing
######################
"""

rule strip_fastq:
    """
    Strip FASTQ read descriptions
    """
    input:
        get_input_fastq 
    output:
        pipe("temp/{sample}/fastqs/stripped_{read}.fastq")
    conda:
        "envs/fastqs.yaml"
    group: 
        "fastqs"
    shell:
        "zcat {input} | sed 's/ .*//' > {output}"

rule match_barcodes:  
    """
    Barcode correction and filtering
    """
    # base rule across technologies
    threads:
        max_threads
    conda:
        "envs/fastqs.yaml"
    group: 
        "fastqs"
    script:
        "scripts/match_barcodes.py"

use rule match_barcodes as match_barcodes_10x with:
    # config["technology"] == "10x"
    input: 
        fq_R1 = "temp/{sample}/fastqs/stripped_R1.fastq",
        fq_R2 = "temp/{sample}/fastqs/stripped_R2.fastq",
        fq_R3 = "temp/{sample}/fastqs/stripped_R3.fastq",
        wl_R2 = lambda w: samples[w.sample]["bc_whitelist"]["R2"]
    output: 
        fastq1_bc = temp("temp/{sample}/fastqs/R1_bc_10x.fastq"),
        fastq2_bc = temp("temp/{sample}/fastqs/R2_bc_10x.fastq"),
        qc_matching = temp("temp/{sample}/fastqs/barcode_matching_10x.tsv")
    params:
        rc_R2 = lambda w: samples[w.sample]["bc_revcomp"]["R2"],
        barcode_dist = lambda w: samples[w.sample]["max_barcode_dist"],
        technology = "10x"

use rule match_barcodes as match_barcodes_multiome with:
    # config["technology"] == "multiome"
    input: 
        fq_R1 = "temp/{sample}/fastqs/stripped_R1.fastq",
        fq_R2 = "temp/{sample}/fastqs/stripped_R2.fastq",
        fq_R3 = "temp/{sample}/fastqs/stripped_R3.fastq",
        wl_R2 = lambda w: samples[w.sample]["bc_whitelist"]["R2"]
    output: 
        fastq1_bc = temp("temp/{sample}/fastqs/R1_bc_multiome.fastq"),
        fastq2_bc = temp("temp/{sample}/fastqs/R2_bc_multiome.fastq"),
        qc_matching = temp("temp/{sample}/fastqs/barcode_matching_multiome.tsv")
    params:
        rc_R2 = lambda w: samples[w.sample]["bc_revcomp"]["R2"],
        barcode_dist = lambda w: samples[w.sample]["max_barcode_dist"],
        technology = "multiome"

use rule match_barcodes as match_barcodes_ren with:
    # config["technology"] == "ren"
    input:
        fq_R1 = "temp/{sample}/fastqs/stripped_R1.fastq",
        fq_R2 = "temp/{sample}/fastqs/stripped_R2.fastq",
        fq_I1 = "temp/{sample}/fastqs/stripped_I1.fastq",
        fq_I2 = "temp/{sample}/fastqs/stripped_I2.fastq",
        wl_I1 = lambda w: samples[w.sample]["bc_whitelist"]["I1"],
        wl_I2 = lambda w: samples[w.sample]["bc_whitelist"]["I2"]
    output: 
        fastq1_bc = temp("temp/{sample}/fastqs/R1_bc_ren.fastq"),
        fastq2_bc = temp("temp/{sample}/fastqs/R2_bc_ren.fastq"),
        qc_matching = temp("temp/{sample}/fastqs/barcode_matching_ren.tsv")
    params:
        rc_I1 = lambda w: samples[w.sample]["bc_revcomp"]["I1"],
        rc_I2 = lambda w: samples[w.sample]["bc_revcomp"]["I2"],
        barcode_dist = lambda w: samples[w.sample]["max_barcode_dist"],
        technology = "ren"

rule output_matching_stats:
    """
    Write barcode matching stats
    """
    input:
        lambda w: f"temp/{w.sample}/fastqs/barcode_matching_{samples[w.sample]['technology']}.tsv",
    output:
        "results/{sample}/qc/barcode_matching.tsv"
    conda:
        "envs/fastqs.yaml"
    group: 
        "fastqs"
    shell:
        "cp {input} {output}"

rule trim_adapter:
    """
    Read adapter trimming
    """
    input:
        fastq1_bc = lambda w: f"temp/{w.sample}/fastqs/R1_bc_{samples[w.sample]['technology']}.fastq",
        fastq2_bc = lambda w: f"temp/{w.sample}/fastqs/R2_bc_{samples[w.sample]['technology']}.fastq"
    output:
        fastq1_trim = "results/{sample}/fastqs/R1_trim.fastq.gz",
        fastq2_trim = "results/{sample}/fastqs/R2_trim.fastq.gz",
        stats = "results/{sample}/qc/trim_adapters.txt"
    log:
        html = "logs/{sample}/fastqs/fastp.html",
        json = "logs/{sample}/fastqs/fastp.json"
    threads:
        max_threads
    conda:
        "envs/fastqs.yaml"
    group: 
        "fastqs"
    shell:
        "fastp -i {input.fastq1_bc} -I {input.fastq2_bc} -o {output.fastq1_trim} -O {output.fastq2_trim}"
        " -h {log.html} -j {log.json} -G -Q -L -w {threads} 2> {output.stats}"


"""
######################
Read mapping
######################
"""

rule bowtie2:
    """
    Read mapping (Bowtie2 aligner)
    """
    input:
        fastq1 = "results/{sample}/fastqs/R1_trim.fastq.gz",
        fastq2 = "results/{sample}/fastqs/R2_trim.fastq.gz"
    output:
        bam_raw = pipe("temp/{sample}/mapping/raw.bam"),
    params:
        k = 1 + config["multimapping"],
        bwt2_idx = config["mapping_index"]
    log:
        "logs/{sample}/mapping/bwt2.log"
    threads:
        max_threads
    conda:
        "envs/mapping.yaml"
    group: 
        "mapping"
    shell:
        "bowtie2 -X 2000 --threads {threads} -x {params.bwt2_idx} "
        "-1 {input.fastq1} -2 {input.fastq2} --sam-append-comment -k {params.k} 2> {log} | "
        "samtools view -u -S - -o {output.bam_raw}"

rule filter_multimappers:
    """
    Remove multimapping reads above threshold
    """
    input:
        "temp/{sample}/mapping/raw.bam"
    output:
        pipe("temp/{sample}/mapping/de-multimap.bam")
    params:
        multimapping = config["multimapping"],
        mmp_path = script_path("scripts/assign_multimappers.py")
    conda:
        "envs/filtering.yaml"
    group: 
        "filtering"
    shell:
        "samtools view -h -f 2 {input} | "
        "python {params.mmp_path} --paired-end -k {params.multimapping} | "
        "samtools view -u -o {output} - "

rule sort_alignments:
    """
    Sort and index alignments
    """
    input: 
        "temp/{sample}/mapping/de-multimap.bam"
    output: 
        bam = "results/{sample}/mapping/raw.bam",
        bai = "results/{sample}/mapping/raw.bam.bai"
    log:
        "logs/{sample}/mapping/sort.log"
    threads:
        max_threads
    conda:
        "envs/mapping.yaml"
    shadow: 
        "minimal"
    group: 
        "mapping"
    shell: 
        "samtools sort -T . {input} -@ {threads} -o {output.bam} 2> {log}; "
        "samtools index {output.bam};"

rule samstats_raw:
    """
    Run SAMStats on raw alignments
    """
    input:
        "results/{sample}/mapping/raw.bam"
    output:
        "results/{sample}/qc/samstats_raw.txt"
    log:
        "logs/{sample}/mapping/samstats_raw.log"
    threads:
        max_threads
    conda:
        "envs/filtering.yaml"
    group: 
        "mapping"
    shadow: 
        "minimal"
    shell:
        "samtools sort -T . -n -@ {threads} {input} -O SAM | " 
        "SAMstats --sorted_sam_file -  --outf {output}"


"""
######################
Alignment filtering
######################
"""

rule remove_mito:
    """
    Remove mitochondrial reads
    """
    input: 
        bam = "results/{sample}/mapping/raw.bam",
        bai = "results/{sample}/mapping/raw.bam.bai"
    output: 
        bam = pipe("temp/{sample}/filter/no_mito.bam"),
        count_mito = temp("temp/{sample}/filter/count_mito.txt"),
        count_no_mito = temp("temp/{sample}/filter/count_no_mito.txt")
    conda:
        "envs/filtering.yaml"
    group: 
        "filtering"
    shell: 
        "samtools view -u "
        # "--unoutput {output.bam} "
        # "-o {output.count_mito} "
        "-o >(samtools view -F 264 -c -o {output.count_mito} -) "
        "-U >(tee {output.bam} | samtools view -F 264 -c -o {output.count_no_mito} -) "
        "{input.bam} chrM"
        # "tee {output.bam} | samtools view -F 264 -c -o {output.count_no_mito} -"

rule frac_mito:
    """
    Calculate fraction of mitochondrial reads
    """
    input: 
        count_mito = "temp/{sample}/filter/count_mito.txt",
        count_no_mito = "temp/{sample}/filter/count_no_mito.txt"
    output: 
        "results/{sample}/qc/frac_mito.txt"
    conda:
        "envs/filtering.yaml"
    group: 
        "filtering"
    shell: 
        "rm=$(<{input.count_mito}); "
        "rn=$(<{input.count_no_mito}); "
        "frac=$((rm / (rm + rn))); "
        "printf \"%d\\t%d\\t%f\" \"$rn\" \"$rm\" \"$frac\" > {output}"

rule assign_primary:
    """
    Assign multimapping reads to primary alignment
    """
    input:
        "temp/{sample}/filter/no_mito.bam"
    output:
        temp("temp/{sample}/filter/primary_align.bam")
    conda:
        "envs/filtering.yaml"
    group: 
        "filtering"
    shell:
        "samtools view -F 1804 -u -o {output} {input} "

rule remove_duplicates_and_index:
    """
    Remove PCR duplicates, report stats, and index
    """
    input:
        "temp/{sample}/filter/primary_align.bam"
    output:
        bam = "results/{sample}/filter/filtered.bam",
        bai = "results/{sample}/filter/filtered.bam.bai",
        markdup_stats = "results/{sample}/qc/markdup.txt",
        pbc_stats = "results/{sample}/qc/pbc_stats.tsv",
    params:
        pbc_script = srcdir("scripts/pbc_stats.py")
    conda:
        "envs/filtering.yaml"
    group: 
        "filtering"
    shell:
        "picard MarkDuplicates I={input} O=/dev/stdout METRICS_FILE={output.markdup_stats} QUIET=true "
        "VALIDATION_STRINGENCY=LENIENT ASSUME_SORTED=true REMOVE_DUPLICATES=false BARCODE_TAG=CB | "
        "tee >(samtools view -F 1804 -f 2 -b - -o {output.bam}) | "
        "samtools view - | python {params.pbc_script} {output.pbc_stats}; "
        "samtools index {output.bam}"

rule samstats_filtered:
    """
    Run SAMStats on filtered alignments
    """
    input:
        "results/{sample}/filter/filtered.bam"
    output:
        "results/{sample}/qc/samstats_filtered.txt"
    log:
        "logs/{sample}/filter/samstats_filtered.log"
    threads:
        max_threads
    conda:
        "envs/filtering.yaml"
    group: 
        "filtering"
    shadow: 
        "minimal"
    shell:
        "samtools sort -T . -n -@ {threads} {input} -O SAM | " 
        "SAMstats --sorted_sam_file -  --outf {output} | "


"""
######################
Fragment file generation
######################
"""

rule bam_to_frag: 
    """
    Convert BAM to fragment file
    """
    input:
        bam = "results/{sample}/filter/filtered.bam",
        bai = "results/{sample}/filter/filtered.bam.bai"
    output:
        pipe("temp/{sample}/fragments/fragments_unsorted.tsv")
    log:
        "logs/{sample}/fragments/sinto.log"
    threads:
        max_threads
    conda:
        "envs/fragments.yaml"
    group: 
        "fragments"
    shell:
        "sinto fragments -b {input.bam} -f {output} " 
        "--min_mapq 0 --max_distance 2000 --min_distance 10 --shift_plus 4 --shift_minus -4 --barcodetag CB --nproc {threads} > {log}"

rule sort_compress_index_fragments:
    """
    Sort fragments and compute counts
    """
    input: 
        "temp/{sample}/fragments/fragments_unsorted.tsv"
    output: 
        fragments = "results/{sample}/fragments/fragments.tsv.gz",
        index = "results/{sample}/fragments/fragments.tsv.gz.tbi",
    params:
        pbc_stats = srcdir("scripts/pbc_stats.awk")
    threads: 
        max_threads
    conda:
        "envs/fragments.yaml"
    group: 
        "fragments"
    shell: 
        "LC_ALL=C sort -k1,1 -k2,2n -k3,3n -k4,4 -t$'\\t' --parallel={threads} {input} | " # Sort the file by chr, start, end, then barcode_id
        "bgzip > {output.fragments}; "
        "tabix --zero-based --preset bed {output.fragments}"

rule tarball_fragments: 
    """
    Create fragments + index tarball
    """
    input:
        frag = "results/{sample}/fragments/fragments.tsv.gz",
        frag_ind = "results/{sample}/fragments/fragments.tsv.gz.tbi"
    output:
        "results/{sample}/fragments/fragments.tar",
    conda:
        "envs/fragments.yaml"
    group: 
        "fragments"
    shell:
        "tar -cf {output} {input.frag} {input.frag_ind}"


"""
######################
ArchR analyses
######################
"""

rule archr_build:
    """
    Preliminary ArchR analyses
    """
    input:
        frag = "results/{sample}/fragments/fragments.tsv.gz",
        frag_ind = "results/{sample}/fragments/fragments.tsv.gz.tbi"
    output:
        qc_dir =temp(directory("temp/{sample}/qc/archr")),
        project_dir = temp(directory("temp/{sample}/archr/project")),
        indicator = touch("temp/{sample}/archr/indicator.txt"),
        qc_ds_pdf = temp("temp/{sample}/archr/project/{sample}/{sample}-Doublet-Summary.pdf"),
        qc_ds_rds = temp("temp/{sample}/archr/project/{sample}/{sample}-Doublet-Summary.rds"),
        qc_frag = temp("temp/{sample}/archr/project/{sample}/{sample}-Fragment_Size_Distribution.pdf"),
        qc_meta = temp("temp/{sample}/archr/project/{sample}/{sample}-Pre-Filter-Metadata.rds"),
        qc_tss = temp("temp/{sample}/archr/project/{sample}/{sample}-TSS_by_Unique_Frags.pdf")
    params:
        sample_name = lambda w: w.sample,
        seed = config["seed"],
        genome = config["genome"]
    log:
        console = "logs/{sample}/archr/console.log",
        arrow_create = "logs/{sample}/archr/arrow_create.log",
        doublets = "logs/{sample}/archr/doublets.log",
        lsi = "logs/{sample}/archr/lsi.log",
        cluster = "logs/{sample}/archr/cluster.log",
        marker_genes = "logs/{sample}/archr/marker_genes.log",
        pseudobulk_rep = "logs/{sample}/archr/pseudobulk_rep.log",
        peak_call = "logs/{sample}/archr/peak_call.log",
        peak_matrix = "logs/{sample}/archr/peak_matrix.log",
        marker_peaks = "logs/{sample}/archr/marker_peaks.log",
        fetch_motif = "logs/{sample}/archr/fetch_motif.log",
        enr_motif = "logs/{sample}/archr/enr_motif.log",
        fetch_tf = "logs/{sample}/archr/fetch_tf.log",
        enr_tf = "logs/{sample}/archr/enr_tf.log",
        save = "logs/{sample}/archr/save.log"
    threads:
        max_threads
    conda:
        "envs/archr.yaml"
    group:
        "archr"
    shadow: 
        "shallow"
    script:
        "scripts/build_archr_project.R"

rule tar_archr_results:
    """
    Create ArchR results archive
    """
    input:
        "temp/{sample}/archr/indicator.txt"
    params:
        project_dir = lambda w: f"temp/{w.sample}/archr/project",
    output:
        "results/{sample}/archr/archr_project.tar.gz"
    conda:
        "envs/archr.yaml"
    group:
        "archr"
    shell:
        "tar -zcf {output} {params.project_dir}"

rule write_archr_qc_pdf:
    """
    Move ArchR QC PDFs
    """
    input:
        qc_ds_pdf = "temp/{sample}/archr/project/{sample}/{sample}-Doublet-Summary.pdf",
        qc_frag = "temp/{sample}/archr/project/{sample}/{sample}-Fragment_Size_Distribution.pdf",
        qc_tss = "temp/{sample}/archr/project/{sample}/{sample}-TSS_by_Unique_Frags.pdf"
    output:
        qc_ds_pdf = "results/{sample}/qc/archr_doublet_summary.pdf",
        qc_frag = "results/{sample}/qc/archr_fragment_size_distribution.pdf",
        qc_tss = "results/{sample}/qc/archr_tss_by_unique_frags.pdf"
    conda:
        "envs/archr.yaml"
    group:
        "archr"
    shell:
        "cp {input.qc_ds_pdf} {output.qc_ds_pdf}; "
        "cp {input.qc_frag} {output.qc_frag}; "
        "cp {input.qc_tss} {output.qc_tss}; "

rule parse_archr_qc:
    """
    Parse ArchR QC RDSs
    """
    input:
        qc_ds_data = "temp/{sample}/archr/project/{sample}/{sample}-Doublet-Summary.rds",
        qc_meta = "temp/{sample}/archr/project/{sample}/{sample}-Pre-Filter-Metadata.rds",
    output:
        qc_ds_data = "results/{sample}/qc/archr_doublet_summary.tsv",
        qc_meta = "results/{sample}/qc/archr_pre_filter_metadata.tsv",
    conda:
        "envs/archr.yaml"
    group:
        "archr"
    script:
        "scripts/parse_archr_qc.R"
